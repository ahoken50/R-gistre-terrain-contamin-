#!/usr/bin/env python3
"""
Script de synchronisation automatique des donn√©es gouvernementales
T√©l√©charge le fichier GPKG depuis donn√©es Qu√©bec, filtre pour Val-d'Or,
d√©tecte les changements et met √† jour Firebase.
"""

import os
import sys
import json
import requests
import geopandas as gpd
import pandas as pd
from datetime import datetime
import firebase_admin
from firebase_admin import credentials, firestore
import tempfile
import logging
import zipfile

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# URL du fichier GPKG
GPKG_URL = "https://www.donneesquebec.ca/recherche/dataset/repertoire-des-terrains-contamines-gtc/resource/09afbfb6-eeac-44fb-86ef-d8b6ce4b739a/download/gtc.gpkg"

# Nom de la municipalit√© √† filtrer
MUNICIPALITY = "Val-d'Or"

# Collections Firebase
GOVERNMENT_DATA_COLLECTION = 'government_data'
SYNC_METADATA_COLLECTION = 'sync_metadata'


def initialize_firebase():
    """Initialiser Firebase Admin SDK"""
    try:
        # Charger les credentials depuis la variable d'environnement
        cred_json = os.environ.get('FIREBASE_CREDENTIALS') or os.environ.get('FIREBASE_SERVICE_ACCOUNT')
        if not cred_json:
            raise ValueError("FIREBASE_SERVICE_ACCOUNT environment variable not set. Please configure the GitHub secret.")
        
        cred_dict = json.loads(cred_json)
        cred = credentials.Certificate(cred_dict)
        
        firebase_admin.initialize_app(cred)
        db = firestore.client()
        
        logger.info("‚úÖ Firebase initialis√© avec succ√®s")
        return db
    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation Firebase: {e}")
        raise


def download_and_extract_gpkg(url):
    """T√©l√©charger et extraire le fichier GPKG depuis le ZIP"""
    try:
        logger.info(f"üì• T√©l√©chargement du fichier ZIP depuis {url}")
        
        response = requests.get(url, stream=True, timeout=300)
        response.raise_for_status()
        
        # Cr√©er un fichier temporaire pour le ZIP
        temp_zip = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
        
        # T√©l√©charger avec barre de progression
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(temp_zip.name, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        if downloaded % (1024 * 1024) == 0:  # Log every MB
                            logger.info(f"T√©l√©chargement: {progress:.1f}%")
        
        logger.info(f"‚úÖ Fichier ZIP t√©l√©charg√©: {temp_zip.name}")
        
        # Extraire le ZIP
        logger.info("üì¶ Extraction du fichier ZIP...")
        temp_dir = tempfile.mkdtemp()
        
        with zipfile.ZipFile(temp_zip.name, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        
        # Trouver le fichier GPKG dans le r√©pertoire extrait
        gpkg_file = None
        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file.endswith('.gpkg'):
                    gpkg_file = os.path.join(root, file)
                    break
            if gpkg_file:
                break
        
        if not gpkg_file:
            raise FileNotFoundError("Aucun fichier .gpkg trouv√© dans l'archive ZIP")
        
        logger.info(f"‚úÖ Fichier GPKG extrait: {gpkg_file}")
        
        # Nettoyer le fichier ZIP
        os.remove(temp_zip.name)
        
        return gpkg_file, temp_dir
    except Exception as e:
        logger.error(f"‚ùå Erreur t√©l√©chargement/extraction: {e}")
        raise


def normalize_text(text):
    """Normaliser le texte pour la comparaison"""
    if pd.isna(text) or text is None:
        return ""
    return str(text).upper().strip()

def belongs_to_valdor(row):
    """V√©rifier si un terrain appartient √† Val-d'Or"""
    address = normalize_text(row.get("ADR_CIV_LIEU", ""))
    mrc = normalize_text(row.get("LST_MRC_REG_ADM", ""))
    
    # Crit√®re 1: L'adresse contient Val-d'Or
    if any(keyword in address for keyword in ["VAL-D'OR", "VAL D'OR", "VALDOR"]):
        return True
    
    # Crit√®re 2: MRC La Vall√©e-de-l'Or ET "VAL" dans l'adresse
    if "LA VALL√âE-DE-L'OR" in mrc and "VAL" in address:
        return True
    
    return False

def filter_valdor_data(gpkg_path):
    """Filtrer et agr√©ger les donn√©es pour Val-d'Or"""
    try:
        logger.info(f"üîç Lecture du fichier GPKG: {gpkg_path}")
        
        # Lister les couches disponibles
        import fiona
        layers = fiona.listlayers(gpkg_path)
        logger.info(f"üìã Couches disponibles: {layers}")
        
        # 1. Lire la couche 'point' pour les localisations
        if 'point' not in layers:
            raise ValueError("Couche 'point' non trouv√©e dans le GPKG")
        
        logger.info("üìñ Lecture de la couche 'point'...")
        points_df = gpd.read_file(gpkg_path, layer='point')
        logger.info(f"üìä Total de points: {len(points_df)}")
        
        # Filtrer pour Val-d'Or
        valdor_points = points_df[points_df.apply(belongs_to_valdor, axis=1)]
        logger.info(f"‚úÖ Points pour Val-d'Or: {len(valdor_points)}")
        
        # 2. Lire la couche 'detailsFiches' pour les d√©tails
        if 'detailsFiches' not in layers:
            logger.warning("‚ö†Ô∏è Couche 'detailsFiches' non trouv√©e, utilisation des donn√©es de base uniquement")
            fiches_df = pd.DataFrame()
        else:
            logger.info("üìñ Lecture de la couche 'detailsFiches'...")
            fiches_df = gpd.read_file(gpkg_path, layer='detailsFiches')
            logger.info(f"üìä Total de fiches: {len(fiches_df)}")
        
        # 3. Agr√©ger les fiches par NO_MEF_LIEU
        if not fiches_df.empty and 'NO_MEF_LIEU' in fiches_df.columns:
            logger.info("üîó Agr√©gation des fiches par terrain...")
            fiches_grouped = fiches_df.groupby('NO_MEF_LIEU').agg({
                'NO_SEQ_DOSSIER': lambda x: ', '.join(map(str, x)),
                'ETAT_REHAB': lambda x: ' | '.join(filter(None, map(str, x))),
                'QUAL_SOLS_AV': lambda x: ', '.join(filter(None, map(str, x))),
                'QUAL_SOLS': lambda x: ', '.join(filter(None, map(str, x))),
                'CONTAM_SOL_EXTRA': lambda x: '; '.join(filter(None, map(str, x))),
                'CONTAM_EAU_EXTRA': lambda x: '; '.join(filter(None, map(str, x))),
                'DATE_CRE_MAJ': lambda x: max(x) if len(x) > 0 else None
            }).reset_index()
        else:
            fiches_grouped = pd.DataFrame()
        
        # 4. Fusionner les donn√©es
        data_list = []
        for idx, row in valdor_points.iterrows():
            no_mef = row.get('NO_MEF_LIEU')
            
            record = {
                'NO_MEF_LIEU': no_mef,
                'LATITUDE': row.get('LATITUDE'),
                'LONGITUDE': row.get('LONGITUDE'),
                'ADR_CIV_LIEU': row.get('ADR_CIV_LIEU'),
                'CODE_POST_LIEU': row.get('CODE_POST_LIEU'),
                'LST_MRC_REG_ADM': row.get('LST_MRC_REG_ADM'),
                'DESC_MILIEU_RECEPT': row.get('DESC_MILIEU_RECEPT'),
                'NB_FICHES': row.get('NB_FICHES', 0)
            }
            
            # Ajouter les d√©tails des fiches si disponibles
            # Check if fiches_grouped is not empty and contains the NO_MEF_LIEU value
            fiche_match = None
            if not fiches_grouped.empty and 'NO_MEF_LIEU' in fiches_grouped.columns:
                # Use boolean indexing to find matching rows
                matching_rows = fiches_grouped[fiches_grouped['NO_MEF_LIEU'] == no_mef]
                if not matching_rows.empty:
                    fiche_match = matching_rows.iloc[0]
            
            if fiche_match is not None:
                record['NO_SEQ_DOSSIER'] = fiche_match.get('NO_SEQ_DOSSIER', '')
                record['ETAT_REHAB'] = fiche_match.get('ETAT_REHAB', '')
                record['QUAL_SOLS_AV'] = fiche_match.get('QUAL_SOLS_AV', '')
                record['QUAL_SOLS'] = fiche_match.get('QUAL_SOLS', '')
                record['CONTAM_SOL_EXTRA'] = fiche_match.get('CONTAM_SOL_EXTRA', '')
                record['CONTAM_EAU_EXTRA'] = fiche_match.get('CONTAM_EAU_EXTRA', '')
                record['DATE_CRE_MAJ'] = str(fiche_match.get('DATE_CRE_MAJ', ''))
                
                # G√©n√©rer les URLs des fiches
                dossiers = str(record['NO_SEQ_DOSSIER']).split(', ')
                record['FICHES_URLS'] = [
                    f"https://www.environnement.gouv.qc.ca/sol/terrains/terrains-contamines/fiche.asp?no={d}"
                    for d in dossiers if d and d != 'nan'
                ]
                
                # V√©rifier si d√©contamin√©
                record['IS_DECONTAMINATED'] = 'Termin√©e' in str(record['ETAT_REHAB'])
            else:
                # Valeurs par d√©faut si pas de fiches
                record['NO_SEQ_DOSSIER'] = ''
                record['ETAT_REHAB'] = ''
                record['QUAL_SOLS_AV'] = ''
                record['QUAL_SOLS'] = ''
                record['CONTAM_SOL_EXTRA'] = ''
                record['CONTAM_EAU_EXTRA'] = ''
                record['DATE_CRE_MAJ'] = ''
                record['FICHES_URLS'] = []
                record['IS_DECONTAMINATED'] = False
            
            # Convertir les valeurs None en string vide et les types non-JSON
            for key, value in record.items():
                if pd.isna(value) or value is None:
                    record[key] = ''
                elif isinstance(value, (list, dict, bool)):
                    pass  # Garder tel quel
                elif not isinstance(value, (int, float, str)):
                    record[key] = str(value)
            
            data_list.append(record)
        
        logger.info(f"‚úÖ {len(data_list)} enregistrements complets pour Val-d'Or")
        
        return data_list
    except Exception as e:
        logger.error(f"‚ùå Erreur filtrage donn√©es: {e}")
        raise


def load_existing_data(db):
    """Charger les donn√©es existantes depuis Firebase"""
    try:
        logger.info("üì• Chargement des donn√©es existantes depuis Firebase")
        
        doc_ref = db.collection(GOVERNMENT_DATA_COLLECTION).document('current')
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            existing_data = data.get('data', [])
            logger.info(f"‚úÖ {len(existing_data)} enregistrements existants charg√©s")
            return existing_data
        else:
            logger.info("‚ÑπÔ∏è Aucune donn√©e existante dans Firebase")
            return []
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement donn√©es existantes: {e}")
        return []


def detect_changes(old_data, new_data):
    """D√©tecter les changements entre anciennes et nouvelles donn√©es"""
    try:
        logger.info("üîç D√©tection des changements...")
        
        # Cr√©er des maps pour recherche rapide
        old_map = {item.get('NO_MEF_LIEU'): item for item in old_data if item.get('NO_MEF_LIEU')}
        new_map = {item.get('NO_MEF_LIEU'): item for item in new_data if item.get('NO_MEF_LIEU')}
        
        # Identifier les nouveaux √©l√©ments
        new_items = [item for item in new_data if item.get('NO_MEF_LIEU') not in old_map]
        
        # Identifier les √©l√©ments retir√©s
        removed_items = [item for item in old_data if item.get('NO_MEF_LIEU') not in new_map]
        
        # Identifier les √©l√©ments modifi√©s
        modified_items = []
        for item in new_data:
            ref = item.get('NO_MEF_LIEU')
            if ref in old_map:
                if json.dumps(old_map[ref], sort_keys=True) != json.dumps(item, sort_keys=True):
                    modified_items.append(item)
        
        changes = {
            'new': new_items,
            'modified': modified_items,
            'removed': removed_items
        }
        
        logger.info(f"üìä Changements d√©tect√©s:")
        logger.info(f"   - Nouveaux: {len(new_items)}")
        logger.info(f"   - Modifi√©s: {len(modified_items)}")
        logger.info(f"   - Retir√©s: {len(removed_items)}")
        
        return changes
    except Exception as e:
        logger.error(f"‚ùå Erreur d√©tection changements: {e}")
        raise


def update_firebase(db, data, changes):
    """Mettre √† jour Firebase avec les nouvelles donn√©es"""
    try:
        logger.info("üíæ Mise √† jour de Firebase...")
        
        # Mettre √† jour les donn√©es gouvernementales
        doc_ref = db.collection(GOVERNMENT_DATA_COLLECTION).document('current')
        doc_ref.set({
            'data': data,
            'lastUpdate': datetime.now().isoformat(),
            'count': len(data)
        })
        
        logger.info(f"‚úÖ {len(data)} enregistrements sauvegard√©s dans Firebase")
        
        # Mettre √† jour les m√©tadonn√©es de synchronisation
        sync_metadata = {
            'last_sync_date': datetime.now().isoformat(),
            'last_sync_status': 'success',
            'changes': {
                'new': len(changes['new']),
                'modified': len(changes['modified']),
                'removed': len(changes['removed'])
            },
            'total_records': len(data),
            'lastUpdate': datetime.now().isoformat()
        }
        
        metadata_ref = db.collection(SYNC_METADATA_COLLECTION).document('current')
        metadata_ref.set(sync_metadata)
        
        logger.info("‚úÖ M√©tadonn√©es de synchronisation sauvegard√©es")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur mise √† jour Firebase: {e}")
        raise


def cleanup_temp_files(file_path, temp_dir=None):
    """Nettoyer les fichiers temporaires"""
    try:
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"üßπ Fichier temporaire supprim√©: {file_path}")
        
        if temp_dir and os.path.exists(temp_dir):
            import shutil
            shutil.rmtree(temp_dir)
            logger.info(f"üßπ R√©pertoire temporaire supprim√©: {temp_dir}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur suppression fichiers temporaires: {e}")


def main():
    """Fonction principale"""
    gpkg_file = None
    temp_dir = None
    
    try:
        logger.info("üöÄ D√©marrage de la synchronisation automatique")
        logger.info(f"üìÖ Date: {datetime.now().isoformat()}")
        
        # 1. Initialiser Firebase
        db = initialize_firebase()
        
        # 2. T√©l√©charger et extraire le fichier GPKG
        gpkg_file, temp_dir = download_and_extract_gpkg(GPKG_URL)
        
        # 3. Filtrer les donn√©es pour Val-d'Or
        new_data = filter_valdor_data(gpkg_file)
        
        # 4. Charger les donn√©es existantes
        old_data = load_existing_data(db)
        
        # 5. D√©tecter les changements
        changes = detect_changes(old_data, new_data)
        
        # 6. Mettre √† jour Firebase
        update_firebase(db, new_data, changes)
        
        logger.info("‚úÖ Synchronisation termin√©e avec succ√®s!")
        
        # Afficher le r√©sum√©
        logger.info("\nüìä R√âSUM√â DE LA SYNCHRONISATION:")
        logger.info(f"   Total d'enregistrements: {len(new_data)}")
        logger.info(f"   Nouveaux: {len(changes['new'])}")
        logger.info(f"   Modifi√©s: {len(changes['modified'])}")
        logger.info(f"   Retir√©s: {len(changes['removed'])}")
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la synchronisation: {e}")
        
        # Enregistrer l'√©chec dans Firebase si possible
        try:
            if 'db' in locals():
                metadata_ref = db.collection(SYNC_METADATA_COLLECTION).document('current')
                metadata_ref.set({
                    'last_sync_date': datetime.now().isoformat(),
                    'last_sync_status': 'error',
                    'error_message': str(e),
                    'lastUpdate': datetime.now().isoformat()
                }, merge=True)
        except:
            pass
        
        return 1
        
    finally:
        # Nettoyer les fichiers temporaires
        cleanup_temp_files(gpkg_file, temp_dir)


if __name__ == '__main__':
    sys.exit(main())