#!/usr/bin/env python3
"""Download the official contaminated lands register with detailsFiches,
filter Val-d'Or data, convert it to JSON/Excel and store it for the web application.

This enhanced version includes rehabilitation status, soil quality, contaminants,
and clickable links to individual fiches.
"""

from __future__ import annotations

import json
import sys
import unicodedata
import zipfile
from datetime import datetime, timezone
from io import BytesIO
from pathlib import Path

import geopandas as gpd
import pandas as pd
import requests
import warnings

warnings.filterwarnings(
    "ignore",
    message="File /vsimem/pyogrio_.* has GPKG application_id, but non conformant file extension",
    category=RuntimeWarning,
)

# Constants -----------------------------------------------------------------
GPKG_ZIP_URL = (
    "https://stqc380donopppdtce01.blob.core.windows.net/donnees-ouvertes/"
    "Repertoire_terrains_contamines/RepertoireTerrainsContamines.gpkg.zip"
)
OUTPUT_JSON = Path("public/data/government-data.json")
OUTPUT_EXCEL = Path("Registre-des-terrains-contamines-Valdor.xlsx")
LAYER_POINT = "point"
LAYER_FICHES = "detailsFiches"
FICHE_URL_TEMPLATE = "https://www.environnement.gouv.qc.ca/sol/terrains/terrains-contamines/fiche.asp?no={}"

ADDRESS_KEYWORDS = ["VAL-D'OR", "VAL D'OR", "VALDOR"]
MRC_KEYWORD = "LA VALL√âE-DE-L'OR"

# Utility functions ---------------------------------------------------------

def log(message: str) -> None:
    print(message)


def normalize_text(value: object) -> str:
    if value is None:
        return ""
    text = str(value).replace("\n", " ").replace("'", "'")
    text = unicodedata.normalize("NFKD", text)
    text = "".join(char for char in text if not unicodedata.combining(char))
    return text.upper()


def download_zip(url: str) -> BytesIO:
    log(f"üì• T√©l√©chargement du registre gouvernemental‚Ä¶\n   ‚Üí {url}")
    response = requests.get(url, timeout=600)
    response.raise_for_status()
    log(f"‚úÖ T√©l√©charg√© ({len(response.content) / (1024 * 1024):.2f} Mo)")
    return BytesIO(response.content)


def extract_gpkg(zip_bytes: BytesIO) -> BytesIO:
    with zipfile.ZipFile(zip_bytes) as archive:
        gpkg_files = [name for name in archive.namelist() if name.endswith(".gpkg")]
        if not gpkg_files:
            raise RuntimeError("Aucun fichier GPKG trouv√© dans l'archive")
        gpkg_name = gpkg_files[0]
        log(f"üì¶ Extraction du fichier {gpkg_name}")
        return BytesIO(archive.read(gpkg_name))


def load_layers(gpkg_bytes: BytesIO) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Load both point and detailsFiches layers from GPKG."""
    log(f"üìÇ Lecture du layer '{LAYER_POINT}'‚Ä¶")
    gdf_point = gpd.read_file(gpkg_bytes, layer=LAYER_POINT)
    log(f"‚úÖ {len(gdf_point)} enregistrements charg√©s (layer point)")
    
    # Reset BytesIO pour relire
    gpkg_bytes.seek(0)
    
    log(f"üìÇ Lecture du layer '{LAYER_FICHES}'‚Ä¶")
    gdf_fiches = gpd.read_file(gpkg_bytes, layer=LAYER_FICHES)
    log(f"‚úÖ {len(gdf_fiches)} enregistrements charg√©s (layer detailsFiches)")
    
    # Supprimer la colonne geometry
    for col in ["geometry"]:
        if col in gdf_point.columns:
            gdf_point = gdf_point.drop(columns=col)
        if col in gdf_fiches.columns:
            gdf_fiches = gdf_fiches.drop(columns=col)
    
    return pd.DataFrame(gdf_point), pd.DataFrame(gdf_fiches)


def filter_valdor(points_df: pd.DataFrame) -> pd.DataFrame:
    log("üîç Filtrage des donn√©es pour la ville de Val-d'Or‚Ä¶")

    def belongs_to_valdor(row) -> bool:
        address = normalize_text(row.get("ADR_CIV_LIEU"))
        mrc = normalize_text(row.get("LST_MRC_REG_ADM"))

        if any(keyword in address for keyword in ADDRESS_KEYWORDS):
            return True

        # Certains enregistrements n'ont pas explicitement Val-d'Or dans l'adresse
        # mais appartiennent √† la MRC de La Vall√©e-de-l'Or.
        if MRC_KEYWORD in mrc and "VAL" in address:
            return True

        return False

    filtered = points_df[points_df.apply(belongs_to_valdor, axis=1)].copy()

    if filtered.empty:
        raise RuntimeError(
            "Aucun enregistrement correspondant √† Val-d'Or n'a √©t√© trouv√© dans le fichier GPKG."
        )

    log(f"‚úÖ {len(filtered)} enregistrements retenus pour Val-d'Or")

    # Trier pour une lecture coh√©rente
    sort_columns = [col for col in ["ADR_CIV_LIEU", "NO_MEF_LIEU"] if col in filtered.columns]
    if sort_columns:
        filtered = filtered.sort_values(by=sort_columns)

    return filtered


def merge_with_fiches(points_df: pd.DataFrame, fiches_df: pd.DataFrame) -> pd.DataFrame:
    """
    Merge point data with fiches data.
    Since a terrain can have multiple fiches, we'll aggregate fiche information.
    """
    log("üîó Fusion des donn√©es avec les fiches‚Ä¶")
    
    # Group fiches by NO_MEF_LIEU
    fiches_grouped = fiches_df.groupby('NO_MEF_LIEU').agg({
        'NO_SEQ_DOSSIER': lambda x: list(x),  # Liste de tous les num√©ros de fiches
        'ETAT_REHAB': lambda x: list(x),  # Liste de tous les √©tats
        'QUAL_SOLS_AV': lambda x: ', '.join([str(v) for v in x if pd.notna(v)]) or None,
        'QUAL_SOLS': lambda x: ', '.join([str(v) for v in x if pd.notna(v)]) or None,
        'CONTAM_SOL_EXTRA': lambda x: '; '.join([str(v) for v in x if pd.notna(v)]) or None,
        'CONTAM_EAU_EXTRA': lambda x: '; '.join([str(v) for v in x if pd.notna(v)]) or None,
        'DATE_CRE_MAJ': lambda x: max([v for v in x if pd.notna(v)], default=None)  # Date la plus r√©cente
    }).reset_index()
    
    # Merge avec les points
    merged = points_df.merge(fiches_grouped, on='NO_MEF_LIEU', how='left')
    
    log(f"‚úÖ {len(merged)} terrains avec leurs fiches")
    
    # G√©n√©rer les URLs des fiches
    def generate_fiche_urls(no_seq_list):
        if no_seq_list is None:
            return []
        if isinstance(no_seq_list, (list, tuple)):
            return [FICHE_URL_TEMPLATE.format(no) for no in no_seq_list if pd.notna(no)]
        if pd.notna(no_seq_list):
            return [FICHE_URL_TEMPLATE.format(no_seq_list)]
        return []
    
    merged['FICHES_URLS'] = merged['NO_SEQ_DOSSIER'].apply(generate_fiche_urls)
    
    # Convertir les listes en strings pour JSON
    merged['NO_SEQ_DOSSIER_LIST'] = merged['NO_SEQ_DOSSIER'].apply(
        lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x) if pd.notna(x) else ''
    )
    merged['ETAT_REHAB_LIST'] = merged['ETAT_REHAB'].apply(
        lambda x: ' | '.join(x) if isinstance(x, list) else str(x) if pd.notna(x) else ''
    )
    
    # D√©tecter si au moins une fiche est "Termin√©e"
    def has_terminated_rehab(etat_list):
        if etat_list is None:
            return False
        if isinstance(etat_list, (list, tuple)):
            return any('Termin√©e' in str(e) for e in etat_list if pd.notna(e))
        if pd.notna(etat_list):
            return 'Termin√©e' in str(etat_list)
        return False
    
    merged['IS_DECONTAMINATED'] = merged['ETAT_REHAB'].apply(has_terminated_rehab)
    
    # Drop les colonnes temporaires avec listes Python (incompatibles JSON)
    merged = merged.drop(columns=['NO_SEQ_DOSSIER', 'ETAT_REHAB'])
    
    # Renommer pour clart√©
    merged = merged.rename(columns={
        'NO_SEQ_DOSSIER_LIST': 'NO_SEQ_DOSSIER',
        'ETAT_REHAB_LIST': 'ETAT_REHAB'
    })
    
    return merged


def convert_dates(df: pd.DataFrame) -> pd.DataFrame:
    for column in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[column]):
            df[column] = df[column].dt.strftime("%Y-%m-%d")
    return df


def save_outputs(df: pd.DataFrame) -> None:
    df = df.fillna("")

    log(f"üíæ Sauvegarde Excel ‚Üí {OUTPUT_EXCEL}")
    df.to_excel(OUTPUT_EXCEL, index=False)

    data_records = df.to_dict(orient="records")
    
    # Statistiques sur les √©tats de r√©habilitation
    decontaminated_count = df['IS_DECONTAMINATED'].sum() if 'IS_DECONTAMINATED' in df.columns else 0
    
    metadata = {
        "source": "Registre des terrains contamin√©s - Gouvernement du Qu√©bec",
        "source_url": GPKG_ZIP_URL,
        "city": "Val-d'Or",
        "layers": [LAYER_POINT, LAYER_FICHES],
        "total_records": len(data_records),
        "decontaminated_count": int(decontaminated_count),
        "last_update": datetime.now(timezone.utc).isoformat(timespec="seconds"),
        "generated_by": "download_gov_data_with_fiches.py",
        "new_fields": [
            "ETAT_REHAB: √âtat de la r√©habilitation",
            "QUAL_SOLS_AV: Qualit√© des sols avant r√©habilitation",
            "QUAL_SOLS: Qualit√© des sols apr√®s r√©habilitation",
            "CONTAM_SOL_EXTRA: Contaminants dans le sol",
            "CONTAM_EAU_EXTRA: Contaminants dans l'eau",
            "NO_SEQ_DOSSIER: Num√©ros de fiches (s√©par√©s par virgule)",
            "FICHES_URLS: URLs des fiches consultables",
            "IS_DECONTAMINATED: True si au moins une fiche indique r√©habilitation termin√©e",
            "DATE_CRE_MAJ: Date de derni√®re cr√©ation/mise √† jour"
        ]
    }

    OUTPUT_JSON.parent.mkdir(parents=True, exist_ok=True)
    payload = {"data": data_records, "metadata": metadata}

    log(f"üìù Sauvegarde JSON ‚Üí {OUTPUT_JSON}")
    OUTPUT_JSON.write_text(json.dumps(payload, ensure_ascii=False, indent=2), "utf-8")

    if data_records:
        log("\nüìÑ Aper√ßu du premier enregistrement:")
        log(json.dumps(data_records[0], ensure_ascii=False, indent=2))
        
        log(f"\nüìä Statistiques:")
        log(f"   Total de terrains: {len(data_records)}")
        log(f"   Terrains d√©contamin√©s: {decontaminated_count}")
        log(f"   Terrains encore contamin√©s: {len(data_records) - decontaminated_count}")


# Main ----------------------------------------------------------------------

def main() -> int:
    log("=" * 78)
    log("üîÑ Actualisation des donn√©es gouvernementales (avec fiches d√©taill√©es)")
    log("=" * 78)
    log(f"‚è∞ D√©but : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        zip_bytes = download_zip(GPKG_ZIP_URL)
        gpkg_bytes = extract_gpkg(zip_bytes)

        points_df, fiches_df = load_layers(gpkg_bytes)
        
        filtered_points = filter_valdor(points_df)
        
        merged_df = merge_with_fiches(filtered_points, fiches_df)
        merged_df = convert_dates(merged_df)
        
        save_outputs(merged_df)

        log("\n‚úÖ Traitement termin√© avec succ√®s")
        log("=" * 78)
        log(f"‚è∞ Fin : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        return 0

    except requests.RequestException as error:
        log(f"‚ùå Erreur r√©seau: {error}")
        return 1
    except Exception as error:  # pylint: disable=broad-except
        log(f"‚ùå Erreur: {error}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
